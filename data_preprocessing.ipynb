{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "datapreprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1NZCWPDr7YtOYArWhshJR3UT3Pk3AgPFm",
      "authorship_tag": "ABX9TyPV4Odu4p221QneMAHuCUHB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lphohmann/BINP37_DL_protein_classification/blob/main/datapreprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdbSyE1eU_Qs"
      },
      "source": [
        "# mount google drive to access files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd drive/MyDrive/DL_project/\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_UQhzdvud5i"
      },
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z3RLemwPJCB"
      },
      "source": [
        "# create the test set and a set from which later training and validation sets will be produced\n",
        "# input file in fasta format\n",
        "input_file = 'data/CH4_database_protein.faa'\n",
        "\n",
        "'''\n",
        "Step 1: load the fasta file with the protein seqeunces and k numbers into a pandas dataframe\n",
        "'''\n",
        "\n",
        "# create dictionary from input fasta file where K number is key and the associated seqs the values\n",
        "def knum_dict(file):\n",
        "    k_dic = {}\n",
        "    with open(file) as input_file:\n",
        "        for line in input_file:\n",
        "            if line.startswith('>'):\n",
        "                header = line.split()[1]\n",
        "                prot_seq = next(input_file).strip().upper()\n",
        "                seq_list = []\n",
        "                seq_list.append(prot_seq)\n",
        "                if header not in k_dic.keys():\n",
        "                    k_dic[header] = list()\n",
        "                else:\n",
        "                    pass\n",
        "                k_dic[header].extend(seq_list)\n",
        "            else:\n",
        "                print(\"This should never appear, if yes the file is in the wrong format\")\n",
        "                break\n",
        "    return k_dic\n",
        "\n",
        "# create the pandas dataframe based on that dictionary\n",
        "def dict_to_df(dict):\n",
        "    x = [item[0] for item in dict.items()] # save all keys in a list\n",
        "    df1 = pd.DataFrame({'Knum': x[0], 'Seq': dict[x[0]]}) # create initial dataframe\n",
        "    for key in x[1:]: # now for the rest of the keys\n",
        "        df2 = pd.DataFrame({'Knum': key, 'Seq': dict[key]}) # for each kay make a dataframe\n",
        "        df1 = pd.concat([df1,df2]) # concatenate them\n",
        "    return df1.reset_index(drop=True) # have to reset index as it is mixed up after concat\n",
        "\n",
        "# running code\n",
        "k_dic = knum_dict(input_file)\n",
        "k_df = dict_to_df(k_dic)\n",
        "\n",
        "# filter out K numbers with less than 500 associated sequences from the dataframes\n",
        "min_seq_cutoff = 500\n",
        "filt_k_df = k_df.groupby(\"Knum\").filter(lambda x: len(x) > min_seq_cutoff)\n",
        "\n",
        "'''\n",
        "Step 2: Stratified random split based (group by) K num column of the pd df into training, validation and test set.\n",
        "'''\n",
        "\n",
        "# the data is first split into a set from which the training and validation set will be created and the test set\n",
        "trainval, test = train_test_split(filt_k_df, test_size=0.1, random_state=42, stratify=filt_k_df[['Knum']])\n",
        "\n",
        "# save as csv files\n",
        "trainval.to_csv('trainval.csv')\n",
        "test.to_csv('test.csv')\n",
        "\n",
        "# This step is moved to the DataBlock step when loading my data for the model\n",
        "# Then the trainval set is split into training and validation set (80/20)\n",
        "#training, validation = train_test_split(trainval, test_size=0.2, random_state=42, stratify=trainval[['Knum']])\n",
        "# save these dfs in .csv files\n",
        "#training.to_csv('training.csv')\n",
        "#validation.to_csv('validation.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPF_BMNyqhia"
      },
      "source": [
        "# check some stats about the data to set some parameters for the transform functions \n",
        "# get average seq length\n",
        "avg_seqlen = filt_k_df['Seq'].apply(len).mean()\n",
        "print(\"average sequence length:\",avg_seqlen) # 260\n",
        "# the max sequence length\n",
        "print(\"max sequence length:\",filt_k_df.Seq.map(len).max()) #2818\n",
        "# the unique characters in the sequences\n",
        "distinc_list = []\n",
        "for key in k_dic:\n",
        "    distinc_list.append(\"{}{}\".format(k_dic[key][0],k_dic[key][1]))\n",
        "distinct_aa = set(''.join(distinc_list))\n",
        "print(distinct_aa) # {'K', 'D', 'N', 'E', 'R', 'A', 'T', 'L', 'I', 'Q', 'C', 'F', 'G', 'W', 'M', 'S', 'H', 'P', 'V', 'Y'} "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
