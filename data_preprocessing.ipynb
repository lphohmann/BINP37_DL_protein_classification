{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocessing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1NZCWPDr7YtOYArWhshJR3UT3Pk3AgPFm",
      "authorship_tag": "ABX9TyNlfQO9qYEsYJfGGbKEh+Cx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lphohmann/BINP37_Research_project/blob/main/data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cLsKYfBj9JD"
      },
      "source": [
        "# Step 1: Mounding drive and importing modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SdbSyE1eU_Qs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57ade4c0-b61f-4a58-8578-4330b5239648"
      },
      "source": [
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive', force_remount=True)\n",
        "# change to the working directory with you data folder present (when putting the path in one string, the directory isnt found)\n",
        "%cd drive/\n",
        "%cd MyDrive/\n",
        "%cd DL_project/ "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/DL_project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_UQhzdvud5i"
      },
      "source": [
        "# imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GeQgVPSdKwV"
      },
      "source": [
        "# Step 2: create a a pandas dataframe from the fasta file with the protein sequences and associated functional annotations (in form of K numbers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-z3RLemwPJCB"
      },
      "source": [
        "# defining functions to load the protein sequences into panda dataframe \n",
        "# create dictionary from input fasta file where K number is key and the associated seqs the values\n",
        "def knum_dict(file):\n",
        "    k_dic = {}\n",
        "    with open(file) as input_file:\n",
        "        for line in input_file:\n",
        "            if line.startswith('>'):\n",
        "                header = line.split()[1]\n",
        "                prot_seq = next(input_file).strip().upper()\n",
        "                seq_list = []\n",
        "                seq_list.append(prot_seq)\n",
        "                if header not in k_dic.keys():\n",
        "                    k_dic[header] = list()\n",
        "                else:\n",
        "                    pass\n",
        "                k_dic[header].extend(seq_list)\n",
        "            else:\n",
        "                print(\"The file is in the wrong format, a single line fasta file is required\")\n",
        "                break\n",
        "    return k_dic\n",
        "\n",
        "# create the pandas dataframe based on that dictionary\n",
        "def dict_to_df(dict):\n",
        "    x = [item[0] for item in dict.items()] # save all keys in a list\n",
        "    df1 = pd.DataFrame({'Knum': x[0], 'Seq': dict[x[0]]}) # create initial dataframe\n",
        "    for key in x[1:]: # now for the rest of the keys\n",
        "        df2 = pd.DataFrame({'Knum': key, 'Seq': dict[key]}) # for each key make a dataframe\n",
        "        df1 = pd.concat([df1,df2]) # concatenate them\n",
        "    return df1.reset_index(drop=True) # have to reset index as it is mixed up after concat"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOyzLH22dCA1"
      },
      "source": [
        "# running code\n",
        "# input file in fasta format\n",
        "input_file = 'data/CH4_database_protein.faa' # specify your input file with the protein sequences in fasta format\n",
        "# create the dataframe with the protein sequences\n",
        "k_dic = knum_dict(input_file)\n",
        "k_df = dict_to_df(k_dic)\n",
        "\n",
        "# filter out K numbers with less than 500 associated sequences from the dataframes\n",
        "min_seq_cutoff = 500\n",
        "filt_k_df = k_df.groupby(\"Knum\").filter(lambda x: len(x) > min_seq_cutoff)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPF_BMNyqhia",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "249ecc1f-0e64-41f8-b9b0-e151bf656092"
      },
      "source": [
        "# looking into parameters about the data which are needed to decide on parameters for the transform functions when loading the inputs for the model\n",
        "# get average seq length\n",
        "avg_seqlen = filt_k_df['Seq'].apply(len).mean()\n",
        "print(\"average sequence length:\",avg_seqlen) # 260\n",
        "# the unique characters in the sequences representing 20 amino acids; used for later encoding the sequences\n",
        "distinc_list = []\n",
        "for key in k_dic:\n",
        "    distinc_list.append(\"{}{}\".format(k_dic[key][0],k_dic[key][1]))\n",
        "distinct_aa = set(''.join(distinc_list))\n",
        "print(distinct_aa) # {'K', 'D', 'N', 'E', 'R', 'A', 'T', 'L', 'I', 'Q', 'C', 'F', 'G', 'W', 'M', 'S', 'H', 'P', 'V', 'Y'}; excluded *"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average sequence length: 260.72583836055605\n",
            "{'Y', 'G', 'A', 'F', 'H', 'E', 'K', 'N', 'L', 'D', 'M', 'P', 'V', 'W', 'Q', 'S', '*', 'C', 'I', 'R', 'T'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNjvbq_yd1KJ"
      },
      "source": [
        "# Step 3: Stratified random split by K number of the dataframe into training and trainval set\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjJFJnsVeESa"
      },
      "source": [
        "# the data is split into a test set and a trainval set (from which the training and validation set will be created)\n",
        "trainval, test = train_test_split(filt_k_df, test_size=0.1, random_state=42, stratify=filt_k_df[['Knum']])\n",
        "\n",
        "# save as csv files in the data directory\n",
        "trainval.to_csv('data/trainval.csv')\n",
        "test.to_csv('data/test.csv')\n",
        "\n",
        "# This step is moved to the DataBlock step when loading my data for the model\n",
        "# Then the trainval set is split into training and validation set (80/20)\n",
        "#training, validation = train_test_split(trainval, test_size=0.2, random_state=42, stratify=trainval[['Knum']])\n",
        "# save these sets in .csv files\n",
        "#training.to_csv('data/training.csv')\n",
        "#validation.to_csv('data/validation.csv')"
      ],
      "execution_count": 6,
      "outputs": []
    }
  ]
}